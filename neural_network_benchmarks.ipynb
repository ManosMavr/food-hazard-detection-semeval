{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification Benchmark\n",
    "In this notebook we are going to do the classification using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "First we are loading the and spliting the data into a train set and devised set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>country</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>hazard-category</th>\n",
       "      <th>product-category</th>\n",
       "      <th>hazard</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>us</td>\n",
       "      <td>Recall Notification: FSIS-024-94</td>\n",
       "      <td>Case Number: 024-94   \\n            Date Opene...</td>\n",
       "      <td>biological</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>smoked sausage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>us</td>\n",
       "      <td>Recall Notification: FSIS-033-94</td>\n",
       "      <td>Case Number: 033-94   \\n            Date Opene...</td>\n",
       "      <td>biological</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>listeria spp</td>\n",
       "      <td>sausage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>us</td>\n",
       "      <td>Recall Notification: FSIS-014-94</td>\n",
       "      <td>Case Number: 014-94   \\n            Date Opene...</td>\n",
       "      <td>biological</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>ham slices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>us</td>\n",
       "      <td>Recall Notification: FSIS-009-94</td>\n",
       "      <td>Case Number: 009-94   \\n            Date Opene...</td>\n",
       "      <td>foreign bodies</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>plastic fragment</td>\n",
       "      <td>thermal processed pork meat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>us</td>\n",
       "      <td>Recall Notification: FSIS-001-94</td>\n",
       "      <td>Case Number: 001-94   \\n            Date Opene...</td>\n",
       "      <td>foreign bodies</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>plastic fragment</td>\n",
       "      <td>chicken breast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day country                             title  \\\n",
       "0  1994      1    7      us  Recall Notification: FSIS-024-94   \n",
       "1  1994      3   10      us  Recall Notification: FSIS-033-94   \n",
       "2  1994      3   28      us  Recall Notification: FSIS-014-94   \n",
       "3  1994      4    3      us  Recall Notification: FSIS-009-94   \n",
       "4  1994      7    1      us  Recall Notification: FSIS-001-94   \n",
       "\n",
       "                                                text hazard-category  \\\n",
       "0  Case Number: 024-94   \\n            Date Opene...      biological   \n",
       "1  Case Number: 033-94   \\n            Date Opene...      biological   \n",
       "2  Case Number: 014-94   \\n            Date Opene...      biological   \n",
       "3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n",
       "4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n",
       "\n",
       "               product-category                  hazard  \\\n",
       "0  meat, egg and dairy products  listeria monocytogenes   \n",
       "1  meat, egg and dairy products            listeria spp   \n",
       "2  meat, egg and dairy products  listeria monocytogenes   \n",
       "3  meat, egg and dairy products        plastic fragment   \n",
       "4  meat, egg and dairy products        plastic fragment   \n",
       "\n",
       "                       product  \n",
       "0               smoked sausage  \n",
       "1                      sausage  \n",
       "2                   ham slices  \n",
       "3  thermal processed pork meat  \n",
       "4               chicken breast  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/incidents_train.csv', index_col=0)\n",
    "trainset, devset = train_test_split(data, test_size=0.2, random_state=4)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "Here we are initializing the Neural network that we will use.\\\n",
    "The code for this was found online and was adjusted for the specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Definition\n",
    "class TextClassifierNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifierNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the vectorizer for the title column\n",
    "I am going to use the same vectorizer that i used in the SVM model for the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "title_tfidf_vect = TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(3, 6), max_df=0.5, min_df=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to train the Neural Network\n",
    "The training function is getting as an input the vectorizer and the name of the column that will train the model.\\\n",
    "Then it is training the model in the trainset and evaluates it in the devset printing out the f_1 scores for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(vectorizer,column):\n",
    "    vectorizer.fit(trainset[column])\n",
    "    X_train = vectorizer.transform(trainset[column]).toarray()\n",
    "    X_dev = vectorizer.transform(devset[column]).toarray()\n",
    "\n",
    "    \n",
    "    # Training Loop\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hidden_dim = 128  # Adjust based on experiments\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Create label encoders for each target column\n",
    "    label_encoders = {}\n",
    "\n",
    "    for label in ('hazard-category', 'product-category', 'hazard', 'product'):\n",
    "        print(label.upper())\n",
    "\n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        le.fit_transform(data[label])\n",
    "        trainset[label] = le.transform(trainset[label])\n",
    "        devset[label] = le.transform(devset[label])\n",
    "        label_encoders[label] = le\n",
    "\n",
    "        # Prepare target labels\n",
    "        y_train = trainset[label].values\n",
    "        y_dev = devset[label].values\n",
    "\n",
    "        # Get number of classes\n",
    "        num_classes = len(le.classes_)\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
    "        y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "        # **Initialize the model for this label**\n",
    "        model = TextClassifierNN(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, len(X_train_tensor), batch_size):\n",
    "                batch_X = X_train_tensor[i:i + batch_size].to(device)\n",
    "                batch_y = y_train_tensor[i:i + batch_size].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on dev set\n",
    "        model.eval()  # **Model is now defined here**\n",
    "        with torch.no_grad():\n",
    "            dev_outputs = model(X_dev_tensor.to(device))\n",
    "            dev_predictions = torch.argmax(dev_outputs, axis=1).cpu().numpy()\n",
    "\n",
    "        # Decode predictions back to string labels\n",
    "        devset['predictions-' + label] = label_encoders[label].inverse_transform(dev_predictions)\n",
    "        devset[label] = label_encoders[label].inverse_transform(devset[label])\n",
    "        print(f'  macro: {f1_score(y_dev, dev_predictions, zero_division=0, average=\"macro\"):.2f}')\n",
    "        print(f'  micro: {f1_score(y_dev, dev_predictions, zero_division=0, average=\"micro\"):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "Epoch 1, Loss: 2.9374\n",
      "Epoch 2, Loss: 2.0666\n",
      "Epoch 3, Loss: 1.3694\n",
      "Epoch 4, Loss: 0.5874\n",
      "Epoch 5, Loss: 0.1745\n",
      "Epoch 6, Loss: 0.0689\n",
      "Epoch 7, Loss: 0.0363\n",
      "Epoch 8, Loss: 0.0225\n",
      "Epoch 9, Loss: 0.0154\n",
      "Epoch 10, Loss: 0.0111\n",
      "  macro: 0.70\n",
      "  micro: 0.84\n",
      "PRODUCT-CATEGORY\n",
      "Epoch 1, Loss: 2.2444\n",
      "Epoch 2, Loss: 0.8472\n",
      "Epoch 3, Loss: 0.1400\n",
      "Epoch 4, Loss: 0.0413\n",
      "Epoch 5, Loss: 0.0181\n",
      "Epoch 6, Loss: 0.0099\n",
      "Epoch 7, Loss: 0.0062\n",
      "Epoch 8, Loss: 0.0043\n",
      "Epoch 9, Loss: 0.0031\n",
      "Epoch 10, Loss: 0.0024\n",
      "  macro: 0.59\n",
      "  micro: 0.75\n",
      "HAZARD\n",
      "Epoch 1, Loss: 5.2973\n",
      "Epoch 2, Loss: 4.4153\n",
      "Epoch 3, Loss: 3.6276\n",
      "Epoch 4, Loss: 2.6175\n",
      "Epoch 5, Loss: 1.3581\n",
      "Epoch 6, Loss: 0.3871\n",
      "Epoch 7, Loss: 0.1124\n",
      "Epoch 8, Loss: 0.0508\n",
      "Epoch 9, Loss: 0.0283\n",
      "Epoch 10, Loss: 0.0181\n",
      "  macro: 0.34\n",
      "  micro: 0.62\n",
      "PRODUCT\n",
      "Epoch 1, Loss: 6.9475\n",
      "Epoch 2, Loss: 6.2187\n",
      "Epoch 3, Loss: 5.3830\n",
      "Epoch 4, Loss: 4.2386\n",
      "Epoch 5, Loss: 2.6933\n",
      "Epoch 6, Loss: 0.9914\n",
      "Epoch 7, Loss: 0.1801\n",
      "Epoch 8, Loss: 0.0565\n",
      "Epoch 9, Loss: 0.0273\n",
      "Epoch 10, Loss: 0.0161\n",
      "  macro: 0.21\n",
      "  micro: 0.40\n"
     ]
    }
   ],
   "source": [
    "training(title_tfidf_vect,'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function that returns the scores for each Subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "  # compute f1 for hazards:\n",
    "  f1_hazards = f1_score(\n",
    "    hazards_true,\n",
    "    hazards_pred,\n",
    "    average='macro'\n",
    "  )\n",
    "\n",
    "  # compute f1 for products:\n",
    "  f1_products = f1_score(\n",
    "    products_true[hazards_pred == hazards_true],\n",
    "    products_pred[hazards_pred == hazards_true],\n",
    "    average='macro'\n",
    "  )\n",
    "\n",
    "  return (f1_hazards + f1_products) / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the title-trained predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Sub-Task 1: 0.655\n",
      "Score Sub-Task 2: 0.299\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score Sub-Task 1: {compute_score(devset['hazard-category'], devset['product-category'], devset['predictions-hazard-category'], devset['predictions-product-category']):.3f}\")\n",
    "print(f\"Score Sub-Task 2: {compute_score(devset['hazard'], devset['product'], devset['predictions-hazard'], devset['predictions-product']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text trained Neural Network\n",
    "Next we are going to do the same thing with the text column.\\\n",
    "First we are initializing the vectorizer for the text column and run the model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf_vect = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), max_df=0.5, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "Epoch 1, Loss: 3.2612\n",
      "Epoch 2, Loss: 1.0725\n",
      "Epoch 3, Loss: 0.1582\n",
      "Epoch 4, Loss: 0.0249\n",
      "Epoch 5, Loss: 0.0084\n",
      "Epoch 6, Loss: 0.0036\n",
      "Epoch 7, Loss: 0.0019\n",
      "Epoch 8, Loss: 0.0012\n",
      "Epoch 9, Loss: 0.0008\n",
      "Epoch 10, Loss: 0.0006\n",
      "  macro: 0.74\n",
      "  micro: 0.92\n",
      "PRODUCT-CATEGORY\n",
      "Epoch 1, Loss: 2.1968\n",
      "Epoch 2, Loss: 0.2833\n",
      "Epoch 3, Loss: 0.0215\n",
      "Epoch 4, Loss: 0.0048\n",
      "Epoch 5, Loss: 0.0018\n",
      "Epoch 6, Loss: 0.0009\n",
      "Epoch 7, Loss: 0.0005\n",
      "Epoch 8, Loss: 0.0003\n",
      "Epoch 9, Loss: 0.0002\n",
      "Epoch 10, Loss: 0.0002\n",
      "  macro: 0.52\n",
      "  micro: 0.71\n",
      "HAZARD\n",
      "Epoch 1, Loss: 6.0265\n",
      "Epoch 2, Loss: 3.3981\n",
      "Epoch 3, Loss: 1.4899\n",
      "Epoch 4, Loss: 0.1636\n",
      "Epoch 5, Loss: 0.0444\n",
      "Epoch 6, Loss: 0.0207\n",
      "Epoch 7, Loss: 0.0118\n",
      "Epoch 8, Loss: 0.0078\n",
      "Epoch 9, Loss: 0.0057\n",
      "Epoch 10, Loss: 0.0044\n",
      "  macro: 0.40\n",
      "  micro: 0.74\n",
      "PRODUCT\n",
      "Epoch 1, Loss: 7.0307\n",
      "Epoch 2, Loss: 4.9953\n",
      "Epoch 3, Loss: 2.9247\n",
      "Epoch 4, Loss: 0.5080\n",
      "Epoch 5, Loss: 0.0338\n",
      "Epoch 6, Loss: 0.0125\n",
      "Epoch 7, Loss: 0.0062\n",
      "Epoch 8, Loss: 0.0037\n",
      "Epoch 9, Loss: 0.0026\n",
      "Epoch 10, Loss: 0.0019\n",
      "  macro: 0.16\n",
      "  micro: 0.32\n"
     ]
    }
   ],
   "source": [
    "training(text_tfidf_vect,'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for the text-trained predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Sub-Task 1: 0.639\n",
      "Score Sub-Task 2: 0.285\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score Sub-Task 1: {compute_score(devset['hazard-category'], devset['product-category'], devset['predictions-hazard-category'], devset['predictions-product-category']):.3f}\")\n",
    "print(f\"Score Sub-Task 2: {compute_score(devset['hazard'], devset['product'], devset['predictions-hazard'], devset['predictions-product']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "In the title trained model we achieved the scores:\\\n",
    " ST-1: 0.655 and ST-2: 0.299\\\n",
    "While in the text trained model we achieved the scores:\\\n",
    " ST-1: 0.639 and ST-2: 0.285\\\n",
    "for the specific train and dev sets we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>country</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>hazard-category</th>\n",
       "      <th>product-category</th>\n",
       "      <th>hazard</th>\n",
       "      <th>product</th>\n",
       "      <th>predictions-hazard-category</th>\n",
       "      <th>predictions-product-category</th>\n",
       "      <th>predictions-hazard</th>\n",
       "      <th>predictions-product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>us</td>\n",
       "      <td>Unilever Issues Allergy Alert on Undeclared Pe...</td>\n",
       "      <td>Unilever is voluntarily recalling a limited nu...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>ices and desserts</td>\n",
       "      <td>peanuts and products thereof</td>\n",
       "      <td>ice cream</td>\n",
       "      <td>allergens</td>\n",
       "      <td>ices and desserts</td>\n",
       "      <td>peanuts and products thereof</td>\n",
       "      <td>ice cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>us</td>\n",
       "      <td>GHSE, LLC Recalls Salads Containing Meat Produ...</td>\n",
       "      <td>WASHINGTON, Oct. 17, 2018 – GHSE, LLC, a Green...</td>\n",
       "      <td>biological</td>\n",
       "      <td>cereals and bakery products</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>corn</td>\n",
       "      <td>biological</td>\n",
       "      <td>fruits and vegetables</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>salads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>us</td>\n",
       "      <td>Moonstruck Chocolate Co. Issues Allergy Alert ...</td>\n",
       "      <td>Moonstruck Chocolate Company of Portland, Oreg...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>confectionery</td>\n",
       "      <td>hazelnut</td>\n",
       "      <td>candies</td>\n",
       "      <td>allergens</td>\n",
       "      <td>cocoa and cocoa preparations, coffee and tea</td>\n",
       "      <td>hazelnut</td>\n",
       "      <td>candies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>us</td>\n",
       "      <td>Arkansas Firm Recalls Breaded Chicken Products...</td>\n",
       "      <td>WASHINGTON, April 2, 2013 - Tyson Foods Inc., ...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>soybeans and products thereof</td>\n",
       "      <td>chicken preparations</td>\n",
       "      <td>allergens</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>soybeans and products thereof</td>\n",
       "      <td>chicken based products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>uk</td>\n",
       "      <td>Orthodox Coconut Palm brand Coconut Juice reca...</td>\n",
       "      <td>Orthodox Coconut Palm brand Coconut Juice is r...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>non-alcoholic beverages</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>coconut juice</td>\n",
       "      <td>allergens</td>\n",
       "      <td>non-alcoholic beverages</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>coconut juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>us</td>\n",
       "      <td>Café Spice GCT Inc., Recalls Ready-To-Eat Chic...</td>\n",
       "      <td>WASHINGTON, Jan. 18, 2018 – Café Spice GCT Inc...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>chicken based products</td>\n",
       "      <td>allergens</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>chicken based products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>uk</td>\n",
       "      <td>Waitrose recalls Waitrose Thrive Fish Pie beca...</td>\n",
       "      <td>Waitrose is recalling Waitrose Thrive Fish Pie...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>seafood</td>\n",
       "      <td>eggs and products thereof</td>\n",
       "      <td>fish products</td>\n",
       "      <td>allergens</td>\n",
       "      <td>prepared dishes and snacks</td>\n",
       "      <td>eggs and products thereof</td>\n",
       "      <td>ready to eat - cook meals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>ca</td>\n",
       "      <td>Picoudi brand microgreens recalled due to Salm...</td>\n",
       "      <td>Food Recall Warning - Picoudi brand microgreen...</td>\n",
       "      <td>biological</td>\n",
       "      <td>fruits and vegetables</td>\n",
       "      <td>salmonella</td>\n",
       "      <td>fresh vegetables</td>\n",
       "      <td>biological</td>\n",
       "      <td>fruits and vegetables</td>\n",
       "      <td>salmonella</td>\n",
       "      <td>alfalfa sprouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>us</td>\n",
       "      <td>Casa Sanchez Foods Recalls</td>\n",
       "      <td>Casa Sanchez Foods of Hayward, CA, is recallin...</td>\n",
       "      <td>biological</td>\n",
       "      <td>soups, broths, sauces and condiments</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>biological</td>\n",
       "      <td>soups, broths, sauces and condiments</td>\n",
       "      <td>listeria monocytogenes</td>\n",
       "      <td>vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>ca</td>\n",
       "      <td>Araliya brand Thala Aluwa recalled due to unde...</td>\n",
       "      <td>Food Recall Warning (Allergen) - Araliya brand...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>confectionery</td>\n",
       "      <td>cereals containing gluten and products thereof</td>\n",
       "      <td>sesame confection</td>\n",
       "      <td>allergens</td>\n",
       "      <td>prepared dishes and snacks</td>\n",
       "      <td>cereals containing gluten and products thereof</td>\n",
       "      <td>curry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1017 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day country  \\\n",
       "2209  2017      5    5      us   \n",
       "3107  2018     10   18      us   \n",
       "3226  2018     12   11      us   \n",
       "802   2013      4    2      us   \n",
       "1460  2015     12   15      uk   \n",
       "...    ...    ...  ...     ...   \n",
       "2551  2018      1   19      us   \n",
       "5441  2021     11   20      uk   \n",
       "4416  2020      8   28      ca   \n",
       "2465  2017     11   15      us   \n",
       "2614  2018      2   15      ca   \n",
       "\n",
       "                                                  title  \\\n",
       "2209  Unilever Issues Allergy Alert on Undeclared Pe...   \n",
       "3107  GHSE, LLC Recalls Salads Containing Meat Produ...   \n",
       "3226  Moonstruck Chocolate Co. Issues Allergy Alert ...   \n",
       "802   Arkansas Firm Recalls Breaded Chicken Products...   \n",
       "1460  Orthodox Coconut Palm brand Coconut Juice reca...   \n",
       "...                                                 ...   \n",
       "2551  Café Spice GCT Inc., Recalls Ready-To-Eat Chic...   \n",
       "5441  Waitrose recalls Waitrose Thrive Fish Pie beca...   \n",
       "4416  Picoudi brand microgreens recalled due to Salm...   \n",
       "2465                         Casa Sanchez Foods Recalls   \n",
       "2614  Araliya brand Thala Aluwa recalled due to unde...   \n",
       "\n",
       "                                                   text hazard-category  \\\n",
       "2209  Unilever is voluntarily recalling a limited nu...       allergens   \n",
       "3107  WASHINGTON, Oct. 17, 2018 – GHSE, LLC, a Green...      biological   \n",
       "3226  Moonstruck Chocolate Company of Portland, Oreg...       allergens   \n",
       "802   WASHINGTON, April 2, 2013 - Tyson Foods Inc., ...       allergens   \n",
       "1460  Orthodox Coconut Palm brand Coconut Juice is r...       allergens   \n",
       "...                                                 ...             ...   \n",
       "2551  WASHINGTON, Jan. 18, 2018 – Café Spice GCT Inc...       allergens   \n",
       "5441  Waitrose is recalling Waitrose Thrive Fish Pie...       allergens   \n",
       "4416  Food Recall Warning - Picoudi brand microgreen...      biological   \n",
       "2465  Casa Sanchez Foods of Hayward, CA, is recallin...      biological   \n",
       "2614  Food Recall Warning (Allergen) - Araliya brand...       allergens   \n",
       "\n",
       "                          product-category  \\\n",
       "2209                     ices and desserts   \n",
       "3107           cereals and bakery products   \n",
       "3226                         confectionery   \n",
       "802           meat, egg and dairy products   \n",
       "1460               non-alcoholic beverages   \n",
       "...                                    ...   \n",
       "2551          meat, egg and dairy products   \n",
       "5441                               seafood   \n",
       "4416                 fruits and vegetables   \n",
       "2465  soups, broths, sauces and condiments   \n",
       "2614                         confectionery   \n",
       "\n",
       "                                              hazard                 product  \\\n",
       "2209                    peanuts and products thereof               ice cream   \n",
       "3107                          listeria monocytogenes                    corn   \n",
       "3226                                        hazelnut                 candies   \n",
       "802                    soybeans and products thereof    chicken preparations   \n",
       "1460                       milk and products thereof           coconut juice   \n",
       "...                                              ...                     ...   \n",
       "2551                       milk and products thereof  chicken based products   \n",
       "5441                       eggs and products thereof           fish products   \n",
       "4416                                      salmonella        fresh vegetables   \n",
       "2465                          listeria monocytogenes               guacamole   \n",
       "2614  cereals containing gluten and products thereof       sesame confection   \n",
       "\n",
       "     predictions-hazard-category  \\\n",
       "2209                   allergens   \n",
       "3107                  biological   \n",
       "3226                   allergens   \n",
       "802                    allergens   \n",
       "1460                   allergens   \n",
       "...                          ...   \n",
       "2551                   allergens   \n",
       "5441                   allergens   \n",
       "4416                  biological   \n",
       "2465                  biological   \n",
       "2614                   allergens   \n",
       "\n",
       "                      predictions-product-category  \\\n",
       "2209                             ices and desserts   \n",
       "3107                         fruits and vegetables   \n",
       "3226  cocoa and cocoa preparations, coffee and tea   \n",
       "802                   meat, egg and dairy products   \n",
       "1460                       non-alcoholic beverages   \n",
       "...                                            ...   \n",
       "2551                  meat, egg and dairy products   \n",
       "5441                    prepared dishes and snacks   \n",
       "4416                         fruits and vegetables   \n",
       "2465          soups, broths, sauces and condiments   \n",
       "2614                    prepared dishes and snacks   \n",
       "\n",
       "                                  predictions-hazard  \\\n",
       "2209                    peanuts and products thereof   \n",
       "3107                          listeria monocytogenes   \n",
       "3226                                        hazelnut   \n",
       "802                    soybeans and products thereof   \n",
       "1460                       milk and products thereof   \n",
       "...                                              ...   \n",
       "2551                       milk and products thereof   \n",
       "5441                       eggs and products thereof   \n",
       "4416                                      salmonella   \n",
       "2465                          listeria monocytogenes   \n",
       "2614  cereals containing gluten and products thereof   \n",
       "\n",
       "            predictions-product  \n",
       "2209                  ice cream  \n",
       "3107                     salads  \n",
       "3226                    candies  \n",
       "802      chicken based products  \n",
       "1460              coconut juice  \n",
       "...                         ...  \n",
       "2551     chicken based products  \n",
       "5441  ready to eat - cook meals  \n",
       "4416            alfalfa sprouts  \n",
       "2465                 vegetables  \n",
       "2614                      curry  \n",
       "\n",
       "[1017 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
