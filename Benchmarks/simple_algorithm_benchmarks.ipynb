{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Machine Learning Algorithms Benchmarks\n",
    "In this notebook we are evaluating some basic machine learning algorithms to choose the one with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "To begin with we are going to load our data and split the training data into the trainset and the devset to train and evaluate our algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>country</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>hazard-category</th>\n",
       "      <th>product-category</th>\n",
       "      <th>hazard</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>au</td>\n",
       "      <td>Marvellous Creations Jelly Popping Candy Beani...</td>\n",
       "      <td>Mondelez Australia Pty Ltd has recalled Marvel...</td>\n",
       "      <td>foreign bodies</td>\n",
       "      <td>cocoa and cocoa preparations, coffee and tea</td>\n",
       "      <td>plastic fragment</td>\n",
       "      <td>chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>us</td>\n",
       "      <td>Request Foods, Inc. Issues Allergy Alert On Un...</td>\n",
       "      <td>Holland, MI - Request Foods, Inc. is recalling...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>other food product / mixed</td>\n",
       "      <td>eggs and products thereof</td>\n",
       "      <td>pasta products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>uk</td>\n",
       "      <td>VBites Foods recalls 'Wot, No Dairy?' desserts</td>\n",
       "      <td>VBites Foods is recalling two 'Wot, No Dairy?'...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>ices and desserts</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>desserts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>ca</td>\n",
       "      <td>Toppits brand Battered Blue Cod Fillet recalle...</td>\n",
       "      <td>Food Recall Warning (Allergen) - Toppits brand...</td>\n",
       "      <td>allergens</td>\n",
       "      <td>seafood</td>\n",
       "      <td>milk and products thereof</td>\n",
       "      <td>cod fillets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>us</td>\n",
       "      <td>Oct 6_ 2006_ Iowa_ Firm Recalls Ground Beef___</td>\n",
       "      <td>WASHINGTON, October 6, 2006 - Jims Market and...</td>\n",
       "      <td>biological</td>\n",
       "      <td>meat, egg and dairy products</td>\n",
       "      <td>escherichia coli</td>\n",
       "      <td>frozen beef patties</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day country  \\\n",
       "1062  2014      7   30      au   \n",
       "1969  2016     11   17      us   \n",
       "1053  2014      7   17      uk   \n",
       "2200  2017      5    1      ca   \n",
       "276   2006     10    6      us   \n",
       "\n",
       "                                                  title  \\\n",
       "1062  Marvellous Creations Jelly Popping Candy Beani...   \n",
       "1969  Request Foods, Inc. Issues Allergy Alert On Un...   \n",
       "1053     VBites Foods recalls 'Wot, No Dairy?' desserts   \n",
       "2200  Toppits brand Battered Blue Cod Fillet recalle...   \n",
       "276      Oct 6_ 2006_ Iowa_ Firm Recalls Ground Beef___   \n",
       "\n",
       "                                                   text hazard-category  \\\n",
       "1062  Mondelez Australia Pty Ltd has recalled Marvel...  foreign bodies   \n",
       "1969  Holland, MI - Request Foods, Inc. is recalling...       allergens   \n",
       "1053  VBites Foods is recalling two 'Wot, No Dairy?'...       allergens   \n",
       "2200  Food Recall Warning (Allergen) - Toppits brand...       allergens   \n",
       "276   WASHINGTON, October 6, 2006 - Jims Market and...      biological   \n",
       "\n",
       "                                  product-category                     hazard  \\\n",
       "1062  cocoa and cocoa preparations, coffee and tea           plastic fragment   \n",
       "1969                    other food product / mixed  eggs and products thereof   \n",
       "1053                             ices and desserts  milk and products thereof   \n",
       "2200                                       seafood  milk and products thereof   \n",
       "276                   meat, egg and dairy products           escherichia coli   \n",
       "\n",
       "                  product  \n",
       "1062            chocolate  \n",
       "1969       pasta products  \n",
       "1053             desserts  \n",
       "2200          cod fillets  \n",
       "276   frozen beef patties  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/incidents_train.csv', index_col=0)\n",
    "trainset, devset = train_test_split(data, test_size=0.2, random_state=2024)\n",
    "trainset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models with the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to train the model and print the f1 scores\n",
    "def title_train(clf):\n",
    "    for label in ('hazard-category', 'product-category', 'hazard', 'product'):\n",
    "        print(label.upper())\n",
    "        clf.fit(trainset.title, trainset[label])\n",
    "\n",
    "        # get development scores:\n",
    "        devset['predictions-' + label] = clf.predict(devset.title)\n",
    "        print(f'  macro: {f1_score(devset[label], devset[\"predictions-\" + label], zero_division=0, average=\"macro\"):.2f}')\n",
    "        print(f'  micro: {f1_score(devset[label], devset[\"predictions-\" + label], zero_division=0, average=\"micro\"):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "The first model we will try will be the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.46\n",
      "  micro: 0.81\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.39\n",
      "  micro: 0.66\n",
      "HAZARD\n",
      "  macro: 0.14\n",
      "  micro: 0.54\n",
      "PRODUCT\n",
      "  macro: 0.07\n",
      "  micro: 0.27\n"
     ]
    }
   ],
   "source": [
    "title_clf_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(2,5), max_df=0.5, min_df=5)),\n",
    "     ('clf', LogisticRegression(max_iter=1000)),\n",
    "    ])\n",
    "\n",
    "\n",
    "title_train(title_clf_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "The next model that we are testing is the k-nearest neighbors model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.57\n",
      "  micro: 0.78\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.39\n",
      "  micro: 0.59\n",
      "HAZARD\n",
      "  macro: 0.19\n",
      "  micro: 0.51\n",
      "PRODUCT\n",
      "  macro: 0.11\n",
      "  micro: 0.26\n"
     ]
    }
   ],
   "source": [
    "title_clf_knn = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(2,5), max_df=0.5, min_df=5)),\n",
    "     ('clf', KNeighborsClassifier()),\n",
    "    ])\n",
    "\n",
    "\n",
    "title_train(title_clf_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results\n",
    "We can see that the KNN model have better scores than the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model\n",
    "Next we are running a support vector machine model with a linear kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.65\n",
      "  micro: 0.83\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.51\n",
      "  micro: 0.71\n",
      "HAZARD\n",
      "  macro: 0.24\n",
      "  micro: 0.59\n",
      "PRODUCT\n",
      "  macro: 0.16\n",
      "  micro: 0.37\n"
     ]
    }
   ],
   "source": [
    "title_clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(2,5), max_df=0.5, min_df=5)),\n",
    "     ('clf', svm.SVC(kernel='linear')),\n",
    "    ])\n",
    "\n",
    "\n",
    "title_train(title_clf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results\n",
    "Since the SVM model has better scores from both Logistic Regression and KNN models we will adjust the parameters of it to achieve even better scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more parameters\n",
    "Here we adjust the C parameter of the model which is a trade-off regularizer and the class_weight parameter to take into consideration the inbalanced classes of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.65\n",
      "  micro: 0.83\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.53\n",
      "  micro: 0.68\n",
      "HAZARD\n",
      "  macro: 0.33\n",
      "  micro: 0.53\n",
      "PRODUCT\n",
      "  macro: 0.17\n",
      "  micro: 0.26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "title_clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(2,5), max_df=0.5, min_df=5)),\n",
    "     ('clf', svm.SVC(kernel='linear', C=1.0, class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "\n",
    "title_train(title_clf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearching\n",
    "We will use the GridSearch to try different parameters for both the tokenizer and the SVM model and find the one with the better f1 macro score for the hazard-category after a 5-fold cross_validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'clf__C': 10, 'clf__kernel': 'linear', 'vect__max_df': 0.5, 'vect__min_df': 3, 'vect__ngram_range': (3, 6)}\n",
      "Best F1-macro score: 0.6448460423434812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(2, 5))),\n",
    "    ('clf', svm.SVC(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'vect__ngram_range': [(2, 5), (3, 6)],  # Experiment with n-grams\n",
    "    'vect__max_df': [0.5, 0.7],             # Test different max document frequency thresholds\n",
    "    'vect__min_df': [3, 5],                 # Test different min document frequency thresholds\n",
    "    'clf__C': [0.1, 1, 10],                 # Regularization parameter for SVM\n",
    "    'clf__kernel': ['linear', 'rbf'],       # Linear or RBF kernel\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring='f1_macro',  # Use F1-macro to optimize for class imbalance\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    verbose=2,           # Show progress\n",
    "    n_jobs=-1            # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit the grid search on one label (e.g., hazard-category)\n",
    "grid_search.fit(trainset.title, trainset['hazard-category'])\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.66\n",
      "  micro: 0.83\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.54\n",
      "  micro: 0.71\n",
      "HAZARD\n",
      "  macro: 0.33\n",
      "  micro: 0.63\n",
      "PRODUCT\n",
      "  macro: 0.21\n",
      "  micro: 0.40\n"
     ]
    }
   ],
   "source": [
    "title_clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='char', ngram_range=(3,6), max_df=0.5, min_df=3)),\n",
    "     ('clf', svm.SVC(kernel='linear', C=10, class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "\n",
    "title_train(title_clf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the scores for the subtasks from the devset with the optimized SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "  # compute f1 for hazards:\n",
    "  f1_hazards = f1_score(\n",
    "    hazards_true,\n",
    "    hazards_pred,\n",
    "    average='macro'\n",
    "  )\n",
    "\n",
    "  # compute f1 for products:\n",
    "  f1_products = f1_score(\n",
    "    products_true[hazards_pred == hazards_true],\n",
    "    products_pred[hazards_pred == hazards_true],\n",
    "    average='macro'\n",
    "  )\n",
    "\n",
    "  return (f1_hazards + f1_products) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Sub-Task 1: 0.449\n",
      "Score Sub-Task 2: 0.121\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score Sub-Task 1: {compute_score(devset['hazard-category'], devset['product-category'], devset['predictions-hazard-category'], devset['predictions-product-category']):.3f}\")\n",
    "print(f\"Score Sub-Task 2: {compute_score(devset['hazard'], devset['product'], devset['predictions-hazard'], devset['predictions-product']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models with the text column\n",
    "Now we will follow the same path but with the text column instead of title to see if we can achieve better results.\n",
    "We have changed the parameters of the vectorizer to vectorize better with the text column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to train the model with text and print the f1 scores\n",
    "def text_train(clf):\n",
    "    for label in ('hazard-category', 'product-category', 'hazard', 'product'):\n",
    "        print(label.upper())\n",
    "        clf.fit(trainset.text, trainset[label])\n",
    "\n",
    "        # get development scores:\n",
    "        devset['predictions-' + label] = clf.predict(devset.text)\n",
    "        print(f'  macro: {f1_score(devset[label], devset[\"predictions-\" + label], zero_division=0, average=\"macro\"):.2f}')\n",
    "        print(f'  micro: {f1_score(devset[label], devset[\"predictions-\" + label], zero_division=0, average=\"micro\"):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.54\n",
      "  micro: 0.85\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.33\n",
      "  micro: 0.59\n",
      "HAZARD\n",
      "  macro: 0.17\n",
      "  micro: 0.64\n",
      "PRODUCT\n",
      "  macro: 0.04\n",
      "  micro: 0.20\n"
     ]
    }
   ],
   "source": [
    "text_clf_lr = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), max_df=0.5, min_df=5)),\n",
    "     ('clf', LogisticRegression(max_iter=1000)),\n",
    "    ])\n",
    "\n",
    "\n",
    "text_train(text_clf_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.53\n",
      "  micro: 0.79\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.33\n",
      "  micro: 0.48\n",
      "HAZARD\n",
      "  macro: 0.23\n",
      "  micro: 0.53\n",
      "PRODUCT\n",
      "  macro: 0.07\n",
      "  micro: 0.20\n"
     ]
    }
   ],
   "source": [
    "text_clf_knn = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), max_df=0.5, min_df=5)),\n",
    "     ('clf', KNeighborsClassifier()),\n",
    "    ])\n",
    "\n",
    "\n",
    "text_train(text_clf_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAZARD-CATEGORY\n",
      "  macro: 0.74\n",
      "  micro: 0.90\n",
      "PRODUCT-CATEGORY\n",
      "  macro: 0.55\n",
      "  micro: 0.68\n",
      "HAZARD\n",
      "  macro: 0.37\n",
      "  micro: 0.73\n",
      "PRODUCT\n",
      "  macro: 0.15\n",
      "  micro: 0.31\n"
     ]
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), max_df=0.5, min_df=5)),\n",
    "     ('clf', svm.SVC(kernel='linear', C=10, class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "\n",
    "text_train(text_clf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the scores of each subtastk with the SVM text-trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Sub-Task 1: 0.648\n",
      "Score Sub-Task 2: 0.278\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score Sub-Task 1: {compute_score(devset['hazard-category'], devset['product-category'], devset['predictions-hazard-category'], devset['predictions-product-category']):.3f}\")\n",
    "print(f\"Score Sub-Task 2: {compute_score(devset['hazard'], devset['product'], devset['predictions-hazard'], devset['predictions-product']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the title-trained and the text-trained models\n",
    "\n",
    "In the title trained model we achieved the scores: ST-1: 0.449 and ST-2: 0.121\\\n",
    "While in the text trained model we achieved the scores: ST-1: 0.648 and ST-2: 0.278\\\n",
    "for the specific train and dev sets we chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold Cross Validation\n",
    "We are going to do a 5-fold cross validation to determine which of the 2 models achieves better f-1 scores for the hazard-category classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title SVM average F1-score: 0.594 ± 0.144\n",
      "Text SVM average F1-score: 0.579 ± 0.060\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "score_title = cross_val_score(title_clf_svm, data.title, data['hazard-category'], cv=5, scoring='f1_macro')\n",
    "score_text = cross_val_score(text_clf_svm, data.text, data['hazard-category'], cv=5, scoring='f1_macro')\n",
    "print(f\"Title SVM average F1-score: {np.mean(score_title):.3f} ± {np.std(score_title):.3f}\")\n",
    "print(f\"Text SVM average F1-score: {np.mean(score_text):.3f} ± {np.std(score_text):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the final model\n",
    "After the 5-fold cross validation we can see that the results of the 2 models are close with the text trained model having a smaller standard deviation.\\\n",
    " So we will choose the text-trained svm model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
